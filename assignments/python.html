<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>GIS595/MEA792: UAV/lidar Data Analytics</title>

<link rel="shortcut icon" href=".././img/favicon.ico" />

<link href="../layout.css" rel="stylesheet" type="text/css" media="screen">
<link href="../style.css" rel="stylesheet" type="text/css" media="screen">

</head>

<body>

<div id="outercontainer">
<div id="container">

<header>
<div id="header-image">
    <h1>GIS595/MEA792:<br>UAV/lidar Data Analytics</h1>
</div>

<nav>
<ul class="nav">
<li><a href="../index.html">Syllabus</a></li>
<li><a href="../schedule.html">Schedule</a></li>
<li><a href="../lectures.html">Lectures</a></li>
<li><a href="../assignments.html">Assignments</a></li>
<li><a href="../projects.html">Projects</a></li>
</ul>
</nav>

</header>

<main>
<!-- This is a generated file. Do not edit. -->
<h2>Using Python and GRASS GIS</h2>

Outline:

<ul>
    <li>Import lidar points.
    <li>Interpolate with parallelization.
    <li>Process multiple maps at once and generate an HTML report.
    <li>Use matplotlib to show terrain profiles.
</ul>

Data:

<ul>
    <li><a href="https://drive.google.com/open?id=0B1AfQGDB8tPXfmZPRHlYSTNIZzdpRVBzay04c0VXUjM4Nzlmbjl5WDlrcGs1bFVLczlTd1k">Lake Wheeler GRASS Location</a>;
        be sure to use separate Mapset for this assignment
    <li>selected LAS tiles from
        <a href="http://fatra.cnr.ncsu.edu/lidar">here</a>
        (see below for the tiles we will need; you can use the provided
        Python code to download them)
</ul>

Tools:

<ul>
    <li>
        Your <a href="http://grass.osgeo.org">GRASS GIS 7</a>
        installation should also include <a href="http://www.liblas.org/">libLAS</a> library
        which is used by GRASS modules v.in.lidar and r.in.lidar
        (standalone GRASS GIS for MS Windows, OSGeo4W and Ubuntu packages contain libLAS).
    </li>
    <li>libLAS installation should include command line tool las2las.
</ul>


<h3>Downloading the data</h3>

In case you don't have the lidar tiles, download them using Python,
otherwise skip this step.

First, create a directory where you want to have the files. Then
set it as current directory in Python:

<pre><code><span class="comment"># use path which applies to you
</span><span class="comment"># on Windows use something like 'D:\\path\\to\\files'
</span>os.chdir('/path/to/las/files')
</code></pre>

Then, download the files using:

<pre><code>import urllib

base_url = 'http://fatra.cnr.ncsu.edu/lidar/'

urllib.urlretrieve(base_url + '0791_005.las', '0791_005.las')
urllib.urlretrieve(base_url + '0791_011.las', '0791_011.las')
urllib.urlretrieve(base_url + '0792_017.las', '0792_017.las')
urllib.urlretrieve(base_url + '0782_020.las', '0782_020.las')
urllib.urlretrieve(base_url + '0781_008.las', '0781_008.las')
</code></pre>

The same could be done in much smarter way using a for loop
and a list of files to download, so you can try that as well.


<h3>Working with command line tools</h3>

The LAS files are in different coordinate system, so we need to change it.

<p>
First, we check if we are in the directory with the files:

<pre><code>print os.getcwd()
</code></pre>

If not, we change directory to where the files are:

<pre><code><span class="comment"># on Windows use 'D:\\path\\to\\files'
</span>os.chdir('/path/to/las/files')
</code></pre>

Now change to coordinate system for all files in the current directory:

<pre><code>import os
from subprocess import call

for f in os.listdir('.'):
    if f.endswith('.las'):
        name, ext = os.path.splitext(f)
        call(['las2las', '--a_srs=EPSG:2264', '--t_srs=EPSG:3358', '-i',
              f, '-o', name + '_spm' + ext])
</code></pre>

In the same step, we could do also some other conversions which are
supported by las2las, for example format conversion.


<h3>Import multiple LAS files to GRASS GIS and merging them</h3>

We are looping through the lidar files
and importing points only in our region of interest.
First set the region in command line:

<pre class="other-code"><code>g.region n=219637 s=219254 w=636730 e=637193 -p
</code></pre>

Now run this script (assuming you are in the directory with the las files):

<pre><code>import os
import grass.script as gscript

region = gscript.region()

vectors = []
for lidar_file in os.listdir('.'):
    if lidar_file.endswith('_spm.las'):
        bbox = gscript.read_command('r.in.lidar', input=lidar_file,
                                    output='foo', flags='g').strip()
        bbox = gscript.parse_key_val(bbox, vsep=' ', val_type=float)
        if (bbox['n'] &lt; region['s'] or bbox['s'] &gt; region['n']
          or bbox['e'] &lt; region['w'] or bbox['w'] &gt; region['e']):
            gscript.info('Skipping tile %s' % lidar_file)
            continue
        name = 'tile_' + lidar_file.rsplit('.', 1)[0]
        vectors.append(name)
        gscript.run_command('v.in.lidar', input=lidar_file, output=name,
                            flags='rt', class_filter=2)
gscript.run_command('v.patch', input=vectors, output='merged_points',
                    flags='b', overwrite=True)
gscript.run_command('g.remove', type='vector', name=vectors, flags='f')
</code></pre>

In case you are having problems with projections but you know that the
projections in fact match, add <code>o</code> flag to <em>r.in.lidar</em>
and <em>v.in.lidar</em> calls.


<h3>DSM interpolation with parallelization</h3>

We will interpolate DEM using <a href="https://grass.osgeo.org/grass70/manuals/libpython/pygrass.modules.grid.html#module-pygrass.modules.grid.grid">
GridModule</a> which allows to run v.surf.rst
in parallel by splitting the region into tiles and interpolating each part
and then patching them back.
First we set the region of our area:
<pre class="other-code"><code>g.region n=219637 s=219254 w=636730 e=637193 res=1 -p
</code></pre>

We import GridModule and compute the size of the tiles
if we want to run it on 4 cores at once:
<pre><code>import grass.script as gscript
from grass.pygrass.modules.grid import GridModule

region = gscript.region()
width = region['cols'] / 2 + 1
height = region['rows'] / 2 + 1
</code></pre>

First try to run it in not parallel mode (debug=True) to see if it is working.
<pre><code>grd = GridModule('v.surf.rst', debug=True, width=width, height=height,
                 overlap=10, processes=4,
                 input='merged_points', elevation='mid_pines_small_dem_1m',
                 tension=30, smooth=1, overwrite=True)
grd.run()
</code></pre>
Note: if you are getting error 'AttributeError: 'VisibleMapset' object has no attribute 'write'',
you have to first set the SEARCH_PATH by going to Settings -> GRASS working environment -> Mapset access
and check anything.

<p>
Then run it in parallel (debug=False) and we can see the time needed reduced significantly:
<pre><code>grd = GridModule('v.surf.rst', debug=False, width=width, height=height,
                 overlap=10, processes=4,
                 input='merged_points', elevation='mid_pines_small_dem_1m',
                 tension=30, smooth=1, overwrite=True)
grd.run()
</code></pre>


<h3>Working with large amount of maps</h3>

Now we will compute difference between raster maps in a list
using nested for loops.

<pre><code>rasters = ['2015_06_20_pix4d_11GCP_dsm', '2015_06_20_DSM_Trimble_11GCP',
           '2015_06_20_DSM_agi_11GCP']
for raster_a in rasters:
    for raster_b in rasters:
        if raster_a == raster_b:
            continue
<span class="comment">        # create the name for the difference raster
</span>        difference = "diff_{a}_{b}".format(a=raster_a.replace('@', '_'),
                                           b=raster_b.replace('@', '_'))
<span class="comment">        # compute difference between the rasters using r.mapcalc
</span>        gscript.mapcalc('{diff} = {a} - {b}'.format(
            a=raster_a, b=raster_b, diff=difference))
<span class="comment">        # set color table to the resulting map
</span>        gscript.run_command('r.colors', map=difference, color='differences')
</code></pre>

Before running the code again, we need to remove the created raster maps
(in the command line):

<pre class="other-code"><code>g.remove type=raster pattern="diff_*" -f
</code></pre>


<h3>Report</h3>

Using HTML and generated images, we can create a report using Python
for the analysis we have made.

<p>
First, we get a list of raster maps we are interested in. We
use <em>g.list</em> wrapper with <code>pattern</code>
option. Then we compute univariate statistics
and we also compute shaded relief for each raster map.
Finally, we render the raster map together with shaded relief using
<em>d.*</em> modules to a PNG file.
Note that we are using <em>d.shade</em> to avoid creating another
raster which would combine the original raster and its shaded relief.

<pre><code><span class="comment"># get list of rasters we are interested in using a search pattern
</span>rasters = gscript.list_strings('raster', pattern='2015_06_*')
<span class="comment"># initialize a dictionary to hold statistics for each raster
</span>stats = {}
<span class="comment"># iterate through the list of rasters
</span>for raster in rasters:
<span class="comment">    # save univariate statistics for raster to dictionary
</span>    stats[raster] = gscript.parse_command('r.univar', map=raster, flags='g')
<span class="comment">    # compute shaded relief, use name of the original raster including mapset
</span>    shaded_relief = raster.replace('@', '_') + '_shade'
    gscript.run_command('r.relief', input=raster, output=shaded_relief,
                        overwrite=True)
    gscript.run_command('r.colors', map=raster, color='elevation')
<span class="comment">    # render the raster (geographical extent follows current region)
</span>    gscript.run_command('d.mon', start='cairo', output=raster + '.png',
                        width=400, height=400, overwrite=True)
    gscript.run_command('d.shade', shade=shaded_relief, color=raster)
    gscript.run_command('d.mon', stop='cairo')
</code></pre>

Now, we create an HTML report from the data collected above.
We also link images created above to the HTML.

<pre><code><span class="comment"># a template for an HTML report for one raster
</span>template = """&lt;h2&gt;Raster map {name}&lt;/h2&gt;
&lt;h3&gt;Statistics&lt;/h3&gt;
&lt;table&gt;
&lt;tr&gt;&lt;td&gt;Mean&lt;/td&gt;&lt;td&gt;{mean}&lt;/td&gt;
&lt;tr&gt;&lt;td&gt;Variance&lt;/td&gt;&lt;td&gt;{var}&lt;/td&gt;
&lt;/table&gt;
&lt;h3&gt;Image&lt;/h3&gt;
&lt;img src="{name}.png"&gt;
"""

<span class="comment"># write to a file using a template
</span>with open('report.html', 'w') as output:
        for raster in sorted(rasters):
            stat = stats[raster]
            output.write(template.format(
                name=raster, mean=stat['mean'], var=stat['variance']))
</code></pre>

To remove the created raster maps, we can use (in the command line):

<pre class="other-code"><code>g.remove type=raster pattern="*_shade" -f
</code></pre>


<h3>Terrain profiles</h3>

Compute a profile for multiple rasters and plot them in <a href="http://matplotlib.org/">matplotlib</a>:

<pre><code>import grass.script as gscript
import matplotlib.pyplot as plt

rasters = ['2015_10_06_DSM_agi_8GCPs', 'mid_pines_lidar2013_dem']
coordinates = [637132,219609,637059,219744]
profiles = {}
distance = []
for raster in rasters:
    profile = gscript.read_command('r.profile', input=raster,
                                   coordinates=coordinates, quiet=True).strip()
<span class="comment">    # parse output
</span>    if not distance:
        for line in profile.splitlines():
            distance.append(float(line.split()[0]))
    profile_elev = []
    for line in profile.splitlines():
        profile_elev.append(float(line.split()[-1]))
    profiles[raster] = profile_elev

for raster in profiles:
    plt.plot(distance, profiles[raster], label=raster)
    plt.legend(loc=0)
plt.show()
</code></pre>

Compute profile statistics using NumPy:

<pre><code>import numpy as np

stats = {}
for raster in profiles:
    profile = np.array(profiles[raster])
    maxim = np.max(profile)
    minim = np.min(profile)
    mean = np.mean(profile)
    stddev = np.std(profile)
    median = np.median(profile)
    roughness = np.std(np.diff(profile))
    stats[raster] = (minim, maxim, mean, stddev, median, roughness)
</code></pre>

And write them into a CSV file:

<pre><code>with open('profile_stats.csv', 'w') as f:
    f.write(','.join(['name', 'minim', 'maxim', 'mean',
                      'stddev', 'median', 'roughness']))
    f.write('\n')
    for raster in profiles:
        f.write(raster + ',')
        f.write(','.join([str(i) for i in stats[raster]]))
        f.write('\n')
</code></pre>


<h3>Tasks and deliverables</h3>

<ul>
    <li>Try the provided code snippets. Modify them as you wish.
    <li>Extent the generated report by computing viewshed from a selected point.
    <li>Deliver the generated report as PDF.
        <ul>
            <li>To get the PDF, print the web page as PDF in a web browser
                (it should be also possible to copy the HTML through
                the clipboard to a word processor like LibreOffice Writer).
            <li>Alternatively, generate report in LaTeX, or another markup,
                and deliver generated PDF.
            <li>In any case, include also the source text file
                (but you don't have to include the individual images).
        </ul>
    <li>Run the script for creating profiles several times with different coordinates.
    <li>Deliver a short version of a normal report with your notes,
        observations, selected figures or screenshots.
</ul>

<hr>

The following parts are optional.


<h3>Downloading all files linked on a website</h3>

<pre><code>import urllib
import re

<span class="comment"># note the slash at the end
</span>base_url = 'http://fatra.cnr.ncsu.edu/lidar/'

tile_file = 'tile_list.html'
<span class="comment"># download the file (in one step)
</span>urllib.urlretrieve(base_url, filename=tile_file)

<span class="comment"># open the file and read it line by line
</span><span class="comment"># we expect to get HTML with a list of filenames in the given directory
</span>with open(tile_file) as html:
    for line in html:
<span class="comment">        # search for the links (a tags with href attribute)
</span><span class="comment">        # we expect one link per line in the HTML source code
</span>        match = re.search(r'&lt;a href="(.*?)"&gt;', line)
        if not match:
            continue
<span class="comment">        # get the content of the href attribute captured before
</span>        filename = match.group(1)
<span class="comment">        # download only files we are interested in
</span>        if not filename.endswith('.las'):
            continue
<span class="comment">        # we expect the link to be relative to our base URL
</span>        urllib.urlretrieve(base_url + filename, filename=filename)
</code></pre>

</main>

<footer>

<nav>
<ul>
<li>
    <a href="https://github.com/ncsu-osgeorel/uav-lidar-analytics-course" title="Fork on GitHub">
        <img src="../img/github_logo.png" alt="GitHub Octocat logo">
    </a>
</li>
<li title="Copyright and license">
    &copy; 2015
    <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA</a>
    <a href="http://geospatial.ncsu.edu/osgeorel/">NCSU OSGeoREL</a>
</li>
</ul>
</nav>

</footer>

</div>
</div>

</body>
</html>

<section>
    <h2>Comparison and fusion of UAS SfM and lidar data</h2>
    <h4>GIS595/MEA792: UAV/lidar Data Analytics</h4>

    <p class="title-foot">
        <a href="http://www.ncsu.edu/" title="North Carolina State University">NCSU</a>
        <a href="http://geospatial.ncsu.edu/osgeorel/" title="NCSU GeoForAll Laboratory">NCSU GeoForAll Lab</a>
        at
        <a href="http://geospatial.ncsu.edu/" title="Center for Geospatial Analytics">Center for Geospatial Analytics</a>
        <br>
    </p>
</section>

<section>
    <h3>Outline</h3>
<p>
<ul>
 <p><li>Analysis of lidar and UAS SfM-derived DSMs differences
 <p><li>Patching and smooth fusion of UAS and lidar DSM
 <p><li>Fusion for viewshed analysis and watershed modeling
</ul>
</section>

<section>
   <h3>Point clouds from lidar and SfM</h3>
<ul>
<li><b>Lidar:</b> 
   <ul>
   <li>measured variable is time of return for each pulse
   <li>georeferencing is based on the position (measured by GPS) 
       and exterior orientation (measured by inertial navigation system INS) of the platform
   <li>(x,y,z) is derived from time, GPS positioning and INS parameters
   </ul>
<p>
<li><b>SfM:</b> 
</ul>
</section>

<section>
   <h3>Point clouds from lidar and SfM</h3>
<ul>
<p>
<li><b>SfM:</b>
    <ul>
    <li>measured variable is reflected energy captured as imagery
    <li>georeferencing can be done entirely from GCPs (GPS is needed)
    <li>(x,y,z) is derived from overlapping images and GCPs
    <li>alternatively: GPS and INS for position and orientation of images and camera parameters
    </ul>
</ul>
<p>Lidar and SfM provide independent set of measurements. 
<p>Georeferencing accuracy depends on GPS and INS 
</section>

<section>
   <h3>Point clouds from lidar and SfM</h3>
<ul>
<li><b>Lidar:</b>
   <ul>
   <li>measured variable is time of return for each pulse
   <li>georeferencing is based on the position (measured by GPS)
       and exterior orientation (measured by inertial navigation system INS) of the platform
   <li>(x,y,z) is derived from time, GPS positioning and INS parameters
   <li>active sensor
   </ul>
<p>
<li><b>SfM:</b>
    <ul>
    <li>measured variable is reflected energy captured as imagery
    <li>georeferencing can be done entirely from GCPs (GPS is needed)
    <li>(x,y,z) is derived from overlapping images and GCPs
    <li>alternatively: GPS and INS for position and orientation of images and camera parameters
    </ul>
</ul>
<p>Lidar and SfM provide independent set of measurements. 
<p>Georeferencing accuracy depends on GPS and INS                  
</section>

<section>
   <h3>Comparison of lidar and SfM point clouds</h3>
<p>
Lidar: passes through vegetation, multiple return, measured profiles in overalpping swaths,
shifts between swaths, condurroy effect
<p>
SfM: very high density, but influenced by cast shadows, difficult to map under vegetation
<p>
Different distribution of error and distortions
<p>
<img width="30%" src="img/fusion_lidar_uav/lidar_points.png">
<img width="65%" src="img/fusion_lidar_uav/uav_points.png">
</section>

<section>
   <h3>Point cloud to DSM</h3>
<ul>
  <li>binning
  <li>meshes / TIN
  <li>spatial interpolation (gridding)
</ul>
<p> Error much smaller than measurement error, because of high point density
<p>
<img width="30%" src="img/fusion_lidar_uav/binned_dsm.png">
<img width="30%" src="img/fusion_lidar_uav/tin_dsm.png">
<img width="30%" src="img/fusion_lidar_uav/interpolated_dsm.png">
</section>

<section>
   <h3>DSM differencing</h3>
<ul>
 <li>Spatial pattern of distortions, systematic errors and noise
 <li>Difference maps between Lidar DSM and UAS SfM DSM (AGISOFT and PIX4D)
</ul>
<p>
<img width="40%" src="img/fusion_lidar_uav/Brendanlidar_agisoft_difference.png">
<img width="45%" src="img/fusion_lidar_uav/Brendanlidar_pix4d_difference.png">
<p style="credit">images by Brendan Harmon</p>
</section>

<section>
   <h3>DSM differencing</h3>
<ul>
 <li>Spatial pattern of distortions, systematic errors and noise
 <li>Difference maps between Lidar DSM and UAS SfM DSM (AGISOFT and PIX4D)
</ul>
<p>
<img width="40%" src="img/fusion_lidar_uav/diff_lid_uavGCPjune.jpg">
<img width="25%" src="img/fusion_lidar_uav/diff_lid_uavGCPjune_gully.jpg">
<img width="35%" src="img/fusion_lidar_uav/gullyDSC06027.jpg">
</section>

<section>
   <h3>Comparing profiles</h3>
<p>
<ul>
<li>identify systematic errors (vertical shifts) 
<li>differences due to vegetation growth, erosion/deposition, ...
</ul>
<img width="35%" src="img/fusion_lidar_uav/Anna_profile_fields.jpg">
<img width="35%" src="img/fusion_lidar_uav/Anna_profile_house.jpg">
</section>


<section>
   <h3>Lidar data sources</h3>
<p>
Public data sources (http in Summary slide):
<ul>
  <li>CLICK: raw point clouds usually in LAS format
  <li>NOAA Digital Coast: costal point clouds with on-fly binning
  <li>NC Floodplain Mapping: bare Earth: points, 20ft DEM and 50ft DEM with carved channels
  <li>NC data portal
  <li>OpenTopography: NCALM data
</ul>
<p>more about lidar in GRASS at http://grasswiki.osgeo.org/wiki/LIDAR
</section>


<div class="parent-page">
    <!-- &#x1f3e0; -->
    <a href="../topics/lidar.html" title="Go to the topic page">&#8962;</a>
</div>
